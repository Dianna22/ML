{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install parfit\n# !pip install --ignore-installed orange3\n# !easy_install -U setuptools","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import f1_score, auc, accuracy_score, confusion_matrix, precision_score, balanced_accuracy_score, recall_score, roc_auc_score, roc_curve\nfrom sklearn import tree, metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import *\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.model_selection import ParameterGrid\n# import parfit.parfit as pf\n# import os\n# print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"PATH_PREFIX = '../input/'\nTWITTER_RELATIVE_500 = PATH_PREFIX + \"Twitter-Relative-Sigma-500.data\"\n\ndef load_dataset(path):\n\tdata = pd.read_csv(path)\n\treturn data[data.columns[:-1]], data[data.columns[-1]]\n\ndata_X, data_Y = load_dataset(TWITTER_RELATIVE_500)\ndata_X = data_X.astype(float)\n\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\n# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = pd.DataFrame(np.column_stack((X_train, Y_train)))\ndata = train_set\nlabel = data.columns[-1]\n\nbuzz = data[label].value_counts()[1]\ninstances = data.describe().iloc[0][0]\n# 38.86933701657458\nnegatives = data.loc[data[label]==0]\npositives = data.loc[data[label]==1]\nprint(len((positives)))\nprint(len(negatives))\nprint(len(negatives)/len(positives))\nprint(len(negatives)/38)\n\nsplits = round(len(negatives)/(round(len(negatives)/len(positives))))\nN = int(len(negatives)/splits)\nframes = [ negatives.iloc[i*splits:(i+1)*splits].copy() for i in range(N+1) ]\n\nframes[-2] = pd.concat([frames[-2],frames[-1]], axis=0)\nframes = frames[:-1]\ndef print_scores(pred_Y):\n  print(f1_score(Y_test, pred_Y))\n  print(roc_auc_score(Y_test, pred_Y, average=\"weighted\"))\n  tn, fp, fn, tp = confusion_matrix(Y_test, pred_Y).ravel()\n  print(balanced_accuracy_score(Y_test, pred_Y))\n  print(tp/(tp+fn))\n  print(precision_score(Y_test, pred_Y))\n  print(tn/(fp+tn))","execution_count":5,"outputs":[{"output_type":"stream","text":"2940\n109625\n37.28741496598639\n2884.8684210526317\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ensemble_predictions(model):\n  models = [model for i in range(len(frames))]\n  for idx,frame in enumerate(frames):\n    dataset = pd.concat([positives, frame], axis=0)\n    models[idx] = models[idx].fit(dataset[dataset.columns[:-1]].values,\n                                  dataset[dataset.columns[-1]].values)\n  predictions = []\n  for model in models:\n    pred = model.predict(X_test)\n    predictions.append(pred)\n  # average predictions\n  predictions = np.matrix(predictions)\n  predictions = np.sum(predictions, axis=0)\n  predictions = np.divide(predictions,len(models))\n  pred_Y = predictions.round()\n  pred_Y = np.array(pred_Y)[0]\n  print_scores(pred_Y)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ensemble_predictions(LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='hinge', max_iter=5000, multi_class='ovr',\n     penalty='l2', random_state=None, tol=1e-05, verbose=0)) #standard scaler\n# 0.381294964028777\n# 0.6867508364499394\n# 0.6867508364499394\n# 0.3897058823529412\n# 0.3732394366197183\n# 0.9837957905469376","execution_count":9,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n","name":"stderr"},{"output_type":"stream","text":"0.381294964028777\n0.6867508364499394\n0.6867508364499394\n0.3897058823529412\n0.3732394366197183\n0.9837957905469376\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ensemble_predictions(KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='kd_tree', p=1)) # standard scaler\n# 0.3510605594835537\n# 0.883402573395537\n# 0.883402573395537\n# 0.8397058823529412\n# 0.22191993781577923\n# 0.9270992644381327","execution_count":10,"outputs":[{"output_type":"stream","text":"0.3510605594835537\n0.883402573395537\n0.883402573395537\n0.8397058823529412\n0.22191993781577923\n0.9270992644381327\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ensemble_predictions(SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n      early_stopping=True, epsilon=0.1, eta0=0.001, fit_intercept=True,\n      l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n      max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n      penalty='none', power_t=0.5, random_state=None, shuffle=True,\n      tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False)) # standard scaler\n# 0.05501840689348275\n# 0.5747032262763091\n# 0.5747032262763091\n# 1.0\n# 0.02828736636299347\n# 0.14940645255261817","execution_count":11,"outputs":[{"output_type":"stream","text":"0.05501840689348275\n0.5747032262763091\n0.5747032262763091\n1.0\n0.02828736636299347\n0.14940645255261817\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ensemble_predictions(KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='kd_tree', p=6)) # standard scaler + norm\n# 0.35069337442218795\n# 0.8820412270217242\n# 0.8820412270217242\n# 0.836764705882353\n# 0.22183235867446394\n# 0.9273177481610954","execution_count":12,"outputs":[{"output_type":"stream","text":"0.35069337442218795\n0.8820412270217242\n0.8820412270217242\n0.836764705882353\n0.22183235867446394\n0.9273177481610954\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ensemble_predictions(KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='kd_tree', p=1)) # standard scaler + norm\n# 0.3510605594835537\n# 0.883402573395537\n# 0.883402573395537\n# 0.8397058823529412\n# 0.22191993781577923\n# 0.9270992644381327","execution_count":13,"outputs":[{"output_type":"stream","text":"0.3510605594835537\n0.883402573395537\n0.883402573395537\n0.8397058823529412\n0.22191993781577923\n0.9270992644381327\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nget_ensemble_predictions(LogisticRegression(C=100))\n# 0.4086799276672694\n# 0.8877665715619875\n# 0.8877665715619873\n# 0.8308823529411765\n# 0.2709832134292566\n# 0.9446507901827981","execution_count":14,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"0.4086799276672694\n0.8877665715619875\n0.8877665715619873\n0.8308823529411765\n0.2709832134292566\n0.9446507901827981\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nget_ensemble_predictions(DecisionTreeClassifier(max_depth=3))\n# 0.3081555834378921\n# 0.9024756133609222\n# 0.9024756133609222\n# 0.9029411764705882\n# 0.18577912254160364\n# 0.9020100502512562","execution_count":17,"outputs":[{"output_type":"stream","text":"0.3081555834378921\n0.9024756133609222\n0.9024756133609222\n0.9029411764705882\n0.18577912254160364\n0.9020100502512562\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data(train, test):\n#   scaler = MinMaxScaler((0, 37505))\n  scaler = StandardScaler() #standard\n  scaler.fit(train)\n  return scaler.transform(train), scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_selection(train, test, comp):\n    pca = PCA(comp)\n    pca = pca.fit(train)\n    return pca.transform(train), pca.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Plots pca"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"X_train, X_test = scale_robust_data(X_train, X_test)\npca = PCA(0.95)\npca_fitted = pca.fit(X_train)\nsns.heatmap(np.log(pca_fitted.inverse_transform(np.eye(pca_fitted.components_.shape[0]))), cmap=\"hot\", cbar=True, cbar_kws={\"orientation\": \"horizontal\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_train, X_test = scale_robust_data(X_train, X_test)\npca_fitted = PCA()\npca_fitted = pca_fitted.fit(X_train)\n# X_train, X_test = pca_fitted.transform(X_train, X_test)\nsns.heatmap(np.log(pca_fitted.inverse_transform(np.eye(X_train.shape[1]))), cmap=\"hot\", cbar=True, cbar_kws={\"orientation\": \"horizontal\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"stratified_kfold = KFold(10, False, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_results(model_builder_fct, scaler, feature_selection, comp, norm, data_X, data_Y, model_name):\n#   ipdb.set_trace(context=10)\n  results = pd.DataFrame(index=range(10))\n  col_name = f\"{model_name}_stratified_kfold_%s\"\n  predictions, Y = [], []\n  f1s, w_aucs, bacc, tpr, tnr, prec = [], [], [], [], [], []\n  for train_idx, test_idx in stratified_kfold.split(data_X):\n    print(\"Fold %s\" % str(len(f1s)))\n    train_X = data_X.iloc[train_idx]\n    test_X = data_X.iloc[test_idx]\n    train_Y, test_Y = data_Y.iloc[train_idx], data_Y.iloc[test_idx]\n\n#     train_X, test_X = scaler(train_X, test_X)\n    if norm:\n      train_X, test_X = normalize(train_X, norm=norm), normalize(test_X, norm=norm)\n    \n    train_X, test_X = scaler(train_X, test_X)\n\n    if feature_selection:\n      train_X, test_X = feature_selection(train_X, test_X, comp)\n    model = model_builder_fct(train_X, train_Y)\n    print(\"Model fit\")\n    pred = model.predict(test_X)\n    predictions.append(pred)\n    Y.append(test_Y)\n#     pred_Y = np.asarray(np.clip(pred,0,1)).round()\n    pred_Y = model.predict(test_X)\n#     print(sum(pred_Y))\n\n#     pred_Y = np.vectorize(lambda x: 0 if x < 0.5 else 1)(pred_Y)\n    f1s.append(f1_score(test_Y, pred_Y))\n    w_aucs.append(roc_auc_score(test_Y, pred_Y, average=\"weighted\"))\n    tn, fp, fn, tp = confusion_matrix(test_Y, pred_Y).ravel()\n    bacc.append(balanced_accuracy_score(test_Y, pred_Y))\n    tpr.append(tp/(tp+fn))\n    tnr.append(tn/(fp+tn))\n#     assert tpr == recall_score(test_Y, pred_Y)\n    prec.append(precision_score(test_Y, pred_Y))\n    \n  f1_df = pd.DataFrame({col_name % (\"F1\"): f1s})\n  auc_df = pd.DataFrame({col_name % (\"AUC\"): w_aucs})\n  bacc_df = pd.DataFrame({col_name % (\"BACC\"): bacc})\n  tpr_df = pd.DataFrame({col_name % (\"TPR(recall)\"): tpr})\n  tnr_df = pd.DataFrame({col_name % (\"TNR\"): tnr})\n  prec = pd.DataFrame({col_name % (\"Precision\"): prec})\n  results = pd.concat([results, f1_df, auc_df, bacc_df, tpr_df, tnr_df, prec], axis=1)\n  return results, (Y, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data_standard(train, test):\n#   scaler = MinMaxScaler((min_data, max_data))\n  scaler = StandardScaler()\n  scaler.fit(train)\n  return scaler.transform(train), scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SGD\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_sgd(train, test):\n  model = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n      early_stopping=True, epsilon=0.1, eta0=0.001, fit_intercept=True,\n      l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n      max_iter=None, n_iter=None, n_iter_no_change=5, n_jobs=None,\n      penalty='none', power_t=0.5, random_state=None, shuffle=True,\n      tol=0.0001, validation_fraction=0.2, verbose=0, warm_start=False)# 0.48384879725085916\n  return model.fit(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled, (y, pred) = test_results(build_sgd, scale_data_standard, None, 'l1', data_X, data_Y, \"SGDNormStandardSC\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled.describe().iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled, (y, pred) = test_results(build_sgd, scale_data_standard, None, 'l1', data_X, data_Y, \"SGDStandardSC\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled.describe().iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled, (y, pred) = test_results(build_sgd, scale_data_standard, None, 'l2', data_X, data_Y, \"SGDNormStandardSC\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled.describe().iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled, (y, pred) = test_results(build_sgd, scale_data_standard, None, 'l2', data_X, data_Y, \"SGDStandardSC\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled.describe().iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled, (y, pred) = test_results(build_sgd, scale_data_standard, None, None, data_X, data_Y, \"SGDStandardSC\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_results_scaled.describe().iloc[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Kneighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nprint(\"Split\")\nX_train, X_test = scale_data(X_train, X_test)\nprint(\"Scaled\")\nX_train, X_test = normalize(X_train), normalize(X_test)\n# X_train, X_test = feature_selection(X_train, X_test)\nprint(\"normalized\")\nmodel = KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='kd_tree', p=6)\nprint(\"fitting model\")\nmodel.fit(X_train, Y_train)\nf1= f1_score(Y_test, model.predict(X_test))\nprint(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(Y_test, model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nprint(\"Split\")\n\nX_train, X_test = scale_data(X_train, X_test)\nprint(\"Scaled\")\n\nX_train, X_test = normalize(X_train, norm='l1'), normalize(X_test, norm='l1')sni\nprint(\"normalized\")\nmodel = KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='kd_tree', p=1)\nmodel.fit(X_train, Y_train)\nprint(\"fitting model\")\nf1= f1_score(Y_test, model.predict(X_test))\nprint(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nX_train, X_test = scale_data(X_train, X_test)\nX_train, X_test = feature_selection(X_train, X_test)\ngrid = {\n    'n_neighbors': [3],\n    'weights': ['distance'],\n    'algorithm': ['kd_tree'],\n    'p': [5,6,7,8,9]\n}\nres = {}\nname = \"%s_%s_%s_%s\"\nfor n in grid['n_neighbors']:\n    for w in grid['weights']:\n        for algo in grid['algorithm']:\n            for p in grid['p']:\n                model = KNeighborsClassifier(n_neighbors=n, weights=w, algorithm=algo, p=p)\n                model.fit(X_train, Y_train)\n                f1= f1_score(Y_test, model.predict(X_test))\n                model_name = name % (str(n), w, algo, str(p))\n                print(model_name,f1)\n                res[model_name] = f1\nprint(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='auto', p=1, metric='manhattan')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nX_train, X_test = scale_data(X_train, X_test)\n# X_train, X_test = feature_selection(X_train, X_test)\n\nX_train, X_test = normalize(X_train, norm='l1'), normalize(X_test, norm='l1')\nprint(\"normalized\")\ngrid = {\n    'n_neighbors': [3],\n    'weights': ['distance'],\n    'algorithm': ['kd_tree'],\n    'p': [1,2,3,4],\n    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n}\nres = {}\nname = \"%s_%s_%s_%s_%s\"\nfor n in grid['n_neighbors']:\n    for w in grid['weights']:\n        for algo in grid['algorithm']:\n            for p in grid['p']:\n                for m in grid['metric']:\n                    model_name = name % (str(n), w, algo, str(p), m)\n#                     print(model_name)\n                    model = KNeighborsClassifier(n_neighbors=n, weights=w, algorithm=algo, p=p, metric=m)\n                    model.fit(X_train, Y_train)\n                    f1= f1_score(Y_test, model.predict(X_test))\n                    print(model_name,f1)\n                    res[model_name] = f1\nprint(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nX_train, X_test = scale_data(X_train, X_test)\nX_train, X_test = feature_selection(X_train, X_test)\ngrid = {\n    'n_neighbors': [1,3,5],\n    'weights': ['distance'],\n    'algorithm': ['kd_tree'],\n    'p': [1,2,3,4,5,6]\n}\nres = {}\nname = \"%s_%s_%s_%s\"\nfor n in grid['n_neighbors']:\n    for w in grid['weights']:\n        for algo in grid['algorithm']:\n            for p in grid['p']:\n                model = KNeighborsClassifier(n_neighbors=n, weights=w, algorithm=algo, p=p)\n                model.fit(X_train, Y_train)\n                f1= f1_score(Y_test, model.predict(X_test))\n                model_name = name % (str(n), w, algo, str(p))\n                print(model_name,f1)\n                res[model_name] = f1\nprint(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM + SGD "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nX_train, X_test = scale_robust_data(X_train, X_test)\nX_train, X_test = pca_transform(X_train, X_test)\ngrid = {\n    'l1_ratio': [0, 0.1, 0.01, 0.2, 0.15],\n    'tol':[1e-3, 1e-4, 1e-5],\n    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n    'class_weight': [None, 'balanced'],\n    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron',\n             'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n    'early_stopping': [True],\n    'validation_fraction': [0.2],\n    'eta0': [0.1, 0.01, 0.001]\n}\nparamGrid = ParameterGrid(grid)\nbestModel, bestScore, allModels, allScores = pf.bestFit(SGDClassifier, paramGrid,\n           X_train, Y_train, X_test, Y_test, \n           metric = f1_score,\n           greater_is_better=True,\n           scoreLabel = \"F1\")\nprint(bestModel, bestScore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegressionCV(Cs=10,cv=5, random_state=0, multi_class='multinomial')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nX_train, X_test = scale_data(X_train, X_test)\nX_train, X_test = feature_selection(X_train, X_test)\ngrid = {\n    'n_neighbors': [1,3,5,7,9,11,13],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'p': [1,2]\n}\nres = {}\nname = \"%s_%s_%s_%s\"\nfor n in grid['n_neighbors']:\n    for w in grid['weights']:\n        for algo in grid['algorithm']:\n            for p in grid['p']:\n                model = KNeighborsClassifier(n_neighbors=n, weights=w, algorithm=algo, p=p)\n                model.fit(X_train, Y_train)\n                f1= f1_score(Y_test, model.predict(X_test))\n                model_name = name % (str(n), w, algo, str(p))\n                print(model_name,f1)\n                res[model_name] = f1\nprint(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nX_train, X_test = scale_robust_data(X_train, X_test)\nX_train, X_test = pca_transform(X_train, X_test)\ngrid = {\n    'cv': [10],\n    'Cs':[10, 12, 14, 16],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'tol':[1e-3, 1e-4, 1e-5],\n    'penalty': ['l2'],\n    'class_weight': [None, 'balanced'],\n}\nparamGrid = ParameterGrid(grid)\nbestModel2, bestScore2, allModels2, allScores2 = pf.bestFit(LogisticRegressionCV, paramGrid,\n           X_train, Y_train, X_test, Y_test, \n           metric = f1_score,\n           greater_is_better=True,\n           scoreLabel = \"F1\")\nprint(bestModel2, bestScore2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nX_train, X_test = scale_data(X_train, X_test)\nX_train, X_test = feature_selection(X_train, X_test)\ngrid = {\n    'cv': [10],\n    'Cs':[10, 12, 14, 16],\n    'solver': ['liblinear', 'saga'],\n    'scoring': ['f1'],\n    'penalty': ['l1'],\n    'class_weight': [None, 'balanced'],\n    'tol':[1e-3, 1e-4, 1e-5],\n}\nparamGrid = ParameterGrid(grid)\nbestModel3, bestScore3, allModels3, allScores3 = pf.bestFit(LogisticRegressionCV, paramGrid,\n           X_train, Y_train, X_test, Y_test, \n           metric = metrics.SCORERS['f1'],\n           greater_is_better=True,\n           scoreLabel = \"F1\")\nprint(bestModel3, bestScore3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.SCORERS['f1']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Log regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import ParameterGrid\nimport parfit.parfit as pf\ngrid = {\n    'C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 10, 100],\n    'penalty': ['l1'],\n    'solver': ['saga', 'warn'],\n    'n_jobs': [-1],\n    'class_weight' :['balanced', None],\n    'tol': [1e-4, 1e-5, 1e-6, 1e-3, 1e-2, 1e-1, 1]\n}\nX, X_tst = scale_data(X_train, X_test)\nparamGrid = ParameterGrid(grid)\nbestModel, bestScore, allModels, allScores = pf.bestFit(LogisticRegression, paramGrid,\n           X, Y_train, X_tst, Y_test, \n           metric = f1_score,\n           greater_is_better=True,\n           scoreLabel = \"F1\")\nprint(bestModel, bestScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import ParameterGrid\nimport parfit.parfit as pf\ngrid = {\n    'C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 10, 100],\n    'penalty': ['l2'],\n    'solver': ['sag', 'newton-cg', 'lbfgs', 'warn'],\n    'n_jobs': [-1],\n    'class_weight' :['balanced', None]\n}\nX, X_tst = scale_data(X_train, X_test)\nX, X_tst = normalize(X), normalize(X_tst)\n\nparamGrid = ParameterGrid(grid)\nbestModel, bestScore, allModels, allScores = pf.bestFit(LogisticRegression, paramGrid,\n           X, Y_train, X_tst, Y_test, \n           metric = f1_score,\n           greater_is_better=True,\n           scoreLabel = \"F1\")\nprint(bestModel, bestScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n          penalty='l2', random_state=None, solver='warn', tol=0.0001,\n          verbose=0, warm_start=False) 0.5320574162679426\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n          verbose=0, warm_start=False) 0.5024248302618817","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nearest centroid"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import NearestCentroid\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\nX_train, X_test = scale_data(X_train, X_test)\nX_train, X_test = feature_selection(X_train, X_test, 37)\ngrid = {\n    'metric': ['euclidean', 'l2', 'l1', 'manhattan', 'cityblock', 'braycurtis', 'canberra', 'chebyshev', 'correlation', 'cosine', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n}\nres = {}\nname = \"%s_%s\"\nfor m in grid['metric']:\n    model = NearestCentroid(metric=m)\n    model.fit(X_train, Y_train)\n    f1= f1_score(Y_test, model.predict(X_test))\n#     model_name = name % (m, str(sh))\n    print(model_name,f1)\n    res[m] = f1\nimport operator\nsorted_x = sorted(res.items(), key=operator.itemgetter(1), reverse=True)\nprint(sorted_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_results(model_builder_fct, scaler, feature_selection, comp, norm, data_X, data_Y, model_name):\n#   ipdb.set_trace(context=10)\n  results = pd.DataFrame(index=range(10))\n  col_name = f\"{model_name}_stratified_kfold_%s\"\n  predictions, Y = [], []\n  f1s, w_aucs, bacc, tpr, tnr, prec = [], [], [], [], [], []\n  for train_idx, test_idx in stratified_kfold.split(data_X):\n    print(\"Fold %s\" % str(len(f1s)))\n    train_X = data_X.iloc[train_idx]\n    test_X = data_X.iloc[test_idx]\n    train_Y, test_Y = data_Y.iloc[train_idx], data_Y.iloc[test_idx]\n    \n    if norm:\n      train_X, test_X = normalize(train_X, norm=norm), normalize(test_X, norm=norm)\n    if scaler:\n      train_X, test_X = scaler(train_X, test_X)\n    if feature_selection:\n      train_X, test_X = feature_selection(train_X, test_X, comp)\n    model = model_builder_fct(train_X, train_Y)\n    print(\"Model fit\")\n    pred = model.predict(test_X)\n    predictions.append(pred)\n    Y.append(test_Y)\n#     pred_Y = np.asarray(np.clip(pred,0,1)).round()\n    pred_Y = model.predict(test_X)\n#     print(sum(pred_Y))\n\n#     pred_Y = np.vectorize(lambda x: 0 if x < 0.5 else 1)(pred_Y)\n    f1s.append(f1_score(test_Y, pred_Y))\n    w_aucs.append(roc_auc_score(test_Y, pred_Y, average=\"weighted\"))\n    tn, fp, fn, tp = confusion_matrix(test_Y, pred_Y).ravel()\n    bacc.append(balanced_accuracy_score(test_Y, pred_Y))\n    tpr.append(tp/(tp+fn))\n    tnr.append(tn/(fp+tn))\n#     assert tpr == recall_score(test_Y, pred_Y)\n    prec.append(precision_score(test_Y, pred_Y))\n    \n  f1_df = pd.DataFrame({col_name % (\"F1\"): f1s})\n  auc_df = pd.DataFrame({col_name % (\"AUC\"): w_aucs})\n  bacc_df = pd.DataFrame({col_name % (\"BACC\"): bacc})\n  tpr_df = pd.DataFrame({col_name % (\"TPR(recall)\"): tpr})\n  tnr_df = pd.DataFrame({col_name % (\"TNR\"): tnr})\n  prec = pd.DataFrame({col_name % (\"Precision\"): prec})\n  results = pd.concat([results, f1_df, auc_df, bacc_df, tpr_df, tnr_df, prec], axis=1)\n  return results, (Y, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_nc_1(train,test):\n    model = NearestCentroid(metric='manhattan')\n    return model.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_PREFIX = '../input/'\nTWITTER_RELATIVE_500 = PATH_PREFIX + \"Twitter-Relative-Sigma-500.data\"\n\ndef load_dataset(path):\n\tdata = pd.read_csv(path)\n\treturn data[data.columns[:-1]], data[data.columns[-1]]\n\ndata_X, data_Y = load_dataset(TWITTER_RELATIVE_500)\ndata_X = data_X.astype(float)\n\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=1)\n# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stratified_kfold = KFold(10, False, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data_standard(train, test):\n#   scaler = MinMaxScaler((0, 37505))\n  scaler = StandardScaler()\n#   scaler = RobustScaler()\n\n  scaler.fit(train)\n  return scaler.transform(train), scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_selection(train, test, comp):\n#     pca = PCA(n_components=comp)\n    pca = PCA(n_components=comp) #-> 17 components\n    pca = pca.fit(train)\n    print((pca.components_).shape[0])\n\n    return pca.transform(train), pca.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def randforest(train, test):\n    model=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n            min_impurity_decrease=0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=77, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n    return model.fit(train,test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results, (y, pred) = test_results(randforest, scale_data_standard, None,None,\"l2\", pd.DataFrame(normalize(data_X)), data_Y, \"NC\") # minmax scaler + pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.describe().iloc[1] # normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.describe().iloc[1] #\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# X_train, X_test = scale_robust_data(X_train, X_test)\n# X_train, X_test = pca_transform(X_train, X_test)\ngrid = {\n    'n_estimators': [2,3,10,20,77],\n    'criterion': [\"entropy\"],\n    'max_features': [0.2, 0.5, 0.7, \"sqrt\", 'log2', 77],\n    'min_impurity_decrease': [0, 0.1,0.01, 1],\n    'class_weight': [None, 'balanced', 'balanced_subsample']\n}\nparamGrid = ParameterGrid(grid)\nbestModel, bestScore, allModels, allScores = pf.bestFit(RandomForestClassifier, paramGrid,\n           X_train, Y_train, X_test, Y_test, \n           metric = f1_score,\n           greater_is_better=True,\n           scoreLabel = \"F1\")\nprint(bestModel, bestScore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_bag(train, test):\n    model = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='kd_tree', p=1),\n#                               DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n#             max_depth=None, max_features=None, max_leaf_nodes=None,\n#             min_impurity_decrease=0.0, min_impurity_split=None,\n#             min_samples_leaf=1, min_samples_split=2,\n#             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n#             splitter='best'),\n         bootstrap=True, bootstrap_features=False, max_features=1.0,\n         max_samples=0.3, n_estimators=5, n_jobs=None, oob_score=False,\n         random_state=None, verbose=0, warm_start=False)\n    return model.fit(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_results, (y, pred) = test_results(build_bag, scale_data_standard, None, None,\"l2\", data_X, data_Y, \"SGDStandardSC\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(true_Y, scores, title = \"ROC curve\"):\n  plt.figure()\n  for i in range(np.shape(true_Y)[0]):\n    Y = true_Y[i]\n    pred = scores[i]\n    fpr, tpr, _ = roc_curve(Y, pred)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr,\n               lw=2, label='ROC curve (weighted_area = %0.2f)' % roc_auc)\n  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n  plt.xlim([0.0, 1.0])\n  plt.ylim([0.0, 1.05])\n  plt.xlabel('False Positive Rate')\n  plt.ylabel('True Positive Rate')\n  plt.title(title)\n  plt.legend()\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y, pred) #bag knn + norm+ scal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y, pred) #bag knn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_results.describe().iloc[1] # bag knn + norm +scal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_results # bag knn + norm +scal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_results.describe().iloc[1] # bag knn\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_results.filter(regex=\"F1\")# bag knn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(y, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_results.describe().iloc[1] # random forest\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_results# random forest\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag_results.filter(regex=\"F1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_test = scale_data_standard(X_train, X_test)\nX_train, X_test = feature_selection(X_train, X_test, 37)\ngrid = {\n    'base_estimator': [\n#         RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n#             max_depth=None, max_features=0.2, max_leaf_nodes=None,\n#             min_impurity_decrease=0, min_impurity_split=None,\n#             min_samples_leaf=1, min_samples_split=2,\n#             min_weight_fraction_leaf=0.0, n_estimators=77, n_jobs=None,\n#             oob_score=False, random_state=None, verbose=0,\n#             warm_start=False)],\n                       KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='kd_tree', p=1)],\n#                        tree.DecisionTreeClassifier(criterion=\"entropy\", class_weight=\"balanced\")],\n    'n_estimators': [5, 10, 20, 77, 100, 120],\n    'max_samples': [0.01, 0.1, 0.2, 0.3],\n    'max_features': [0.2, 0.5, 0.7, 1.0],\n    'bootstrap_features': [True, False],\n}\nparamGrid = ParameterGrid(grid)\nbestModel, bestScore, allModels, allScores = pf.bestFit(BaggingClassifier, paramGrid,\n           X_train, Y_train, X_test, Y_test, \n           metric = f1_score,\n           greater_is_better=True,\n           scoreLabel = \"F1\")\nprint(bestModel, bestScore)\n\n\n# BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n#             max_depth=None, max_features=None, max_leaf_nodes=None,\n#             min_impurity_decrease=0.0, min_impurity_split=None,\n#             min_samples_leaf=1, min_samples_split=2,\n#             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n#             splitter='best'),\n#          bootstrap=True, bootstrap_features=False, max_features=1.0,\n#          max_samples=0.3, n_estimators=5, n_jobs=None, oob_score=False,\n#          random_state=None, verbose=0, warm_start=False) 0.48144624167459565\n# BaggingClassifier(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n#             max_depth=None, max_features=0.2, max_leaf_nodes=None,\n#             min_impurity_decrease=0, min_impurity_split=None,\n#             min_samples_leaf=1, min_samples_split=2,\n#             min_weight_fraction_leaf=0.0, n_estimators=77, n_jobs=None,\n#             oob_score=False, random_state=None, verbose=0,\n#             warm_start=False),\n#          bootstrap=True, bootstrap_features=False, max_features=1.0,\n#          max_samples=0.3, n_estimators=5, n_jobs=None, oob_score=False,\n#          random_state=None, verbose=0, warm_start=False) 0.5565565565565566","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RULE"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_PREFIX = '../input/'\nTWITTER_RELATIVE_500 = PATH_PREFIX + \"Twitter-Relative-Sigma-500.data\"\n\ndef load_data(path):\n\tdata = pd.read_csv(path)\n\treturn pd.DataFrame(data)\n\ndata = load_data(TWITTER_RELATIVE_500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef series2table(series, variable):\n    if series.dtype is np.dtype(\"int\") or series.dtype is np.dtype(\"float\"):\n        series = series.values[:, np.newaxis]\n        return Orange.data.Table(series)\n    else:\n        series = series.astype('category').cat.codes.reshape((-1,1))\n        return Orange.data.Table(series)\n\ndef df2table(tdomain, df):\n#     tdomain = df2domain(df)\n    ttables = [series2table(df.iloc[:,i], tdomain[i]) for i in range(len(df.columns))]\n    ttables = np.array(ttables).reshape((len(df.columns),-1)).transpose()\n    return Orange.data.Table(tdomain , ttables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import Orange\ndomain = Orange.data.Domain([Orange.data.DiscreteVariable(name) for name in data.columns])\ntbl = df2table(domain, data)\n\n# print(tbl.class_var)\n# tbl = Orange.data.Table.from_file(PATH_PREFIX + \"data/Twitter-Relative-Sigma-500_rule.txt\")\nlearner = Orange.classification.CN2UnorderedLearner()\n\n# consider up to 10 solution streams at one time\nlearner.rule_finder.search_algorithm.beam_width = 10\n\n# continuous value space is constrained to reduce computation time\nlearner.rule_finder.search_strategy.constrain_continuous = True\n\n# found rules must cover at least 15 examples\nlearner.rule_finder.general_validator.min_covered_examples = 15\n\n# found rules may combine at most 2 selectors (conditions)\nlearner.rule_finder.general_validator.max_rule_length = 2\n\nclassifier = learner(tbl)\n\n# Cross validating results\nres = Orange.evaluation.testing.CrossValidation(tbl, [learner], k=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}